#Q1.


#1
import pandas as pd
import numpy as np
df1 = pd.read_csv('cars_hw.csv')

no_owner_mapping = {'1st': 1, '2nd': 2, '3rd': 3}
df1['No_of_Owners'] = df1['No_of_Owners'].replace(no_owner_mapping)
df1 = df1.iloc[:, 1:]

#2
import seaborn as sns
import matplotlib.pyplot as plt

#price_summary = df1['Price'].describe()
#plt.figure(figsize = (8, 5))
#sns.kdeplot(df1['Price'], shade = True)
#plt.title('Kernel Density Plot for Price')
#plt.xlabel('Price')
#plt.ylabel('Density')
#plt.show()

#price_by_make = df1.groupby('Make')['Price'].describe()
#plt.figure(figsize = (12, 8))
#sns.kdeplot(data = df1, x = 'Price', hue = 'Make', common_norm = False, multiple = 'stack', fill = True)
#plt.title('Grouped Kernel Density Plot by Make')
#plt.xlabel('Price')
#plt.ylabel('Density')
#plt.legend(title = 'Make', loc = 'upper right')
#plt.show()

# response...

print(df1.dtypes)

#3
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

train_set, test_set = train_test_split(df1, test_size = 0.2, random_state = 42)

#4
numeric_variables = ['Make_Year', 'Mileage_Run', 'No_of_Owners', 'Seating_Capacity']
X_numeric = df1[numeric_variables]
y = df1['Price']
X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)
model_numeric = LinearRegression()
model_numeric.fit(X_train_numeric, y_train)
y_pred_numeric = model_numeric.predict(X_test_numeric)
r2_numeric = r2_score(y_test, y_pred_numeric)
rmse_numeric = np.sqrt(mean_squared_error(y_test, y_pred_numeric))
print("Numeric R-squared:" + str(r2_numeric))
print("Numeric Root mean squared error:" + str(rmse_numeric))

categorical_columns = ['Make', 'Color', 'Body_Type', 'Fuel_Type', 'Transmission', 'Transmission_Type']
df1_encoded = pd.get_dummies(df1, columns=categorical_columns, drop_first=True)
X_categorical = df1_encoded.drop(columns=['Price'])
X_train_categorical, X_test_categorical, y_train, y_test = train_test_split(X_categorical, y, test_size=0.2, random_state=42)
model_categorical = LinearRegression()
model_categorical.fit(X_train_categorical, y_train)
y_pred_categorical = model_categorical.predict(X_test_categorical)
r2_categorical = r2_score(y_test, y_pred_categorical)
rmse_categorical = np.sqrt(mean_squared_error(y_test, y_pred_categorical))
print("Categorical R-squared:" + str(r2_categorical))
print("Categorical Root mean squared error:" + str(rmse_categorical))

X_combined = pd.concat([X_numeric, X_categorical], axis=1)
X_train_combined, X_test_combined, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)
model_combined = LinearRegression()
model_combined.fit(X_train_combined, y_train)
y_pred_combined = model_combined.predict(X_test_combined)
r2_combined = r2_score(y_test, y_pred_combined)
rmse_combined = np.sqrt(mean_squared_error(y_test, y_pred_combined))
print("Combined R-squared:" + str(r2_combined))
print("Combined Root mean squared error:" + str(rmse_combined))

# response...

#5
from sklearn.preprocessing import PolynomialFeatures

degrees = range(1, 11)
r2_scores = []
rmse_values = []

for degree in degrees:
    # Create polynomial features
    poly = PolynomialFeatures(degree=degree)
    X_train_poly = poly.fit_transform(X_train_categorical)
    X_test_poly = poly.transform(X_test_categorical)
    model_poly = LinearRegression()
    model_poly.fit(X_train_poly, y_train)
    y_pred_poly = model_poly.predict(X_test_poly)
    r2_poly = r2_score(y_test, y_pred_poly)
    rmse_poly = np.sqrt(mean_squared_error(y_test, y_pred_poly))
    r2_scores.append(r2_poly)
    rmse_values.append(rmse_poly)
    print(f"Degree {degree}: R-squared (R2): {r2_poly:.4f}, RMSE: {rmse_poly:.2f}")

negative_r2_degree = None
for i, r2_score in enumerate(r2_scores):
    if r2_score < 0:
        negative_r2_degree = degrees[i]
        break
print(f"R-squared goes negative at degree {negative_r2_degree}")

best_degree = degrees[np.argmax(r2_scores)]
best_r2 = r2_scores[np.argmax(r2_scores)]
best_rmse = rmse_values[np.argmax(r2_scores)]
print(f"Best degree (highest R2) is {best_degree} with R-squared: {best_r2:.4f} and RMSE: {best_rmse:.2f}")

#6
import seaborn as sns

y_pred_best = model_combined.predict(X_test_combined)

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_best, alpha=0.5)
plt.title("Predicted vs. True Values")
plt.xlabel("True Values")
plt.ylabel("Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.show()

residuals = y_test - y_pred_best

plt.figure(figsize=(8, 6))
sns.kdeplot(residuals, shade=True)
plt.title("Residuals Kernel Density Plot")
plt.xlabel("Residuals")
plt.ylabel("Density")
plt.show()

#Strengths:

#Weaknesses:

#7
from sklearn.tree import DecisionTreeRegressor

max_depths = range(1, 21)

r2_scores = []
rmse_values = []

for max_depth in max_depths:
    tree_reg = DecisionTreeRegressor(max_depth=max_depth, random_state=42)
    tree_reg.fit(X_train_combined, y_train)
    y_pred_tree = tree_reg.predict(X_test_combined)
    r2_tree = r2_score(y_test, y_pred_tree)
    rmse_tree = np.sqrt(mean_squared_error(y_test, y_pred_tree))
    r2_scores.append(r2_tree)
    rmse_values.append(rmse_tree)
    print(f"Max Depth {max_depth}: R-squared (R2): {r2_tree:.4f}, RMSE: {rmse_tree:.2f}")
best_depth = max_depths[np.argmax(r2_scores)]
best_r2_tree = r2_scores[np.argmax(r2_scores)]
best_rmse_tree = rmse_values[np.argmax(r2_scores)]
print(f"Best max depth for tree: {best_depth} with R-squared: {best_r2_tree:.4f} and RMSE: {best_rmse_tree:.2f}")

#8
best_tree_reg = DecisionTreeRegressor(max_depth=best_depth, random_state=42)
best_tree_reg.fit(X_train_combined, y_train)
y_pred_tree_best = best_tree_reg.predict(X_test_combined)

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_tree_best, alpha=0.5)
plt.title("Predicted vs. True Values (Best Tree)")
plt.xlabel("True Values")
plt.ylabel("Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.show()

residuals_tree_best = y_test - y_pred_tree_best

plt.figure(figsize=(8, 6))
sns.kdeplot(residuals_tree_best, shade=True)
plt.title("Residuals Kernel Density Plot (Best Tree)")
plt.xlabel("Residuals")
plt.ylabel("Density")
plt.show()

#9
# Response...
