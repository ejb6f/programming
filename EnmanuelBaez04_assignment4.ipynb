#Q1.


#1
import pandas as pd
import numpy as np
df1 = pd.read_csv('cars_hw.csv')

no_owner_mapping = {'1st': 1, '2nd': 2, '3rd': 3}
df1['No_of_Owners'] = df1['No_of_Owners'].replace(no_owner_mapping)
df1 = df1.iloc[:, 1:]

#2
import seaborn as sns
import matplotlib.pyplot as plt

price_summary = df1['Price'].describe()
plt.figure()
sns.kdeplot(df1['Price'], shade = True)
plt.title('Kernel Density Plot for Price')
plt.xlabel('Price')
plt.ylabel('Density')
plt.show()

price_by_make = df1.groupby('Make')['Price'].describe()
plt.figure()
sns.kdeplot(data = df1, x = 'Price', hue = 'Make', common_norm = False, multiple = 'stack', fill = True)
plt.title('Grouped Kernel Density Plot by Make')
plt.xlabel('Price')
plt.ylabel('Density')
plt.legend(title = 'Make', loc = 'upper right')
plt.show()

# response...

print(df1.dtypes)

#3
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

train_set, test_set = train_test_split(df1, test_size = 0.2, random_state = 42)

#4
numeric_variables = ['Make_Year', 'Mileage_Run', 'No_of_Owners', 'Seating_Capacity']
X_numeric = df1[numeric_variables]
y = df1['Price']
X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)
model_numeric = LinearRegression()
model_numeric.fit(X_train_numeric, y_train)
y_pred_numeric = model_numeric.predict(X_test_numeric)
r2_numeric = r2_score(y_test, y_pred_numeric)
rmse_numeric = np.sqrt(mean_squared_error(y_test, y_pred_numeric))
print("Numeric R-squared:" + str(r2_numeric))
print("Numeric Root mean squared error:" + str(rmse_numeric))

categorical_columns = ['Make', 'Color', 'Body_Type', 'Fuel_Type', 'Transmission', 'Transmission_Type']
df1_encoded = pd.get_dummies(df1, columns=categorical_columns, drop_first=True)
X_categorical = df1_encoded.drop(columns=['Price'])
X_train_categorical, X_test_categorical, y_train, y_test = train_test_split(X_categorical, y, test_size=0.2, random_state=42)
model_categorical = LinearRegression()
model_categorical.fit(X_train_categorical, y_train)
y_pred_categorical = model_categorical.predict(X_test_categorical)
r2_categorical = r2_score(y_test, y_pred_categorical)
rmse_categorical = np.sqrt(mean_squared_error(y_test, y_pred_categorical))
print("Categorical R-squared:" + str(r2_categorical))
print("Categorical Root mean squared error:" + str(rmse_categorical))

X_combined = pd.concat([X_numeric, X_categorical], axis=1)
X_train_combined, X_test_combined, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)
model_combined = LinearRegression()
model_combined.fit(X_train_combined, y_train)
y_pred_combined = model_combined.predict(X_test_combined)
r2_combined = r2_score(y_test, y_pred_combined)
rmse_combined = np.sqrt(mean_squared_error(y_test, y_pred_combined))
print("Combined R-squared:" + str(r2_combined))
print("Combined Root mean squared error:" + str(rmse_combined))

# response...

#5

from sklearn.preprocessing import PolynomialFeatures

degrees = range(1, 3)
r2_scores = []
rmse_values = []

for degree in degrees:
    poly = PolynomialFeatures(degree=degree)
    X_train_poly = poly.fit_transform(X_train_categorical)
    X_test_poly = poly.transform(X_test_categorical)
    model_poly = LinearRegression()
    model_poly.fit(X_train_poly, y_train)
    y_pred_poly = model_poly.predict(X_test_poly)
    r2_poly = r2_score(y_test, y_pred_poly)
    rmse_poly = np.sqrt(mean_squared_error(y_test, y_pred_poly))
    r2_scores.append(r2_poly)
    rmse_values.append(rmse_poly)
    print(f"Degree {degree}: R-squared (R2): {r2_poly:.4f}, RMSE: {rmse_poly:.2f}")

negative_r2_degree = None
for i, r2_score in enumerate(r2_scores):
    if r2_score < 0:
        negative_r2_degree = degrees[i]
        break
print(f"R-squared goes negative at degree {negative_r2_degree}")

# response...

#6

poly_reg = make_pipeline(PolynomialFeatures(1), LinearRegression())
poly_reg.fit(X_train_combined, y_train)
y_pred_poly = poly_reg.predict(X_test_combined)
plt.figure()
plt.scatter(y_test, y_pred_poly, alpha=0.5)
plt.title(f"Predicted vs. True Values for Polynomial Degree {1}")
plt.xlabel("True Values")
plt.ylabel("Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.show()

residuals_poly = y_test - y_pred_poly

plt.figure()
sns.kdeplot(residuals_poly, shade=True)
plt.title(f"Residuals Kernel Density Plot for Polynomial Degree {1}")
plt.xlabel("Residuals")
plt.ylabel("Density")
plt.show()

# response...

#7

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error

depth_range = range(1, 16) 
r2_scores = []
rmse_scores = []

for depth in depth_range:
    tree_reg = DecisionTreeRegressor(max_depth=depth, random_state=42)
    tree_reg.fit(X_train_combined, y_train)
    y_pred_tree = tree_reg.predict(X_test_combined)
    r2_tree_result = r2_score(y_test, y_pred_tree)
    rmse_tree_result = np.sqrt(mean_squared_error(y_test, y_pred_tree))
    r2_scores.append(r2_tree_result)
    rmse_scores.append(rmse_tree_result)

    print(f"Max Depth {depth}: R-squared: {r2_tree_result}, RMSE: {rmse_tree_result}")

best_depth_index = np.argmax(r2_scores)
best_depth = depth_range[best_depth_index]

print(f"The best results are obtained with a max depth of {best_depth}.")
print(f"Best R-squared: {r2_scores[best_depth_index]}")
print(f"Best RMSE: {rmse_scores[best_depth_index]}")

# response...

#8

best_tree = DecisionTreeRegressor(max_depth=9, random_state=42)
best_tree.fit(X_train_combined, y_train)
y_pred_best_tree = best_tree.predict(X_test_combined)

plt.figure()
plt.scatter(y_test, y_pred_best_tree, alpha=0.5)
plt.title("Predicted vs. True Values for the Best Decision Tree")
plt.xlabel("True Values")
plt.ylabel("Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.show()

residuals_best_tree = y_test - y_pred_best_tree

plt.figure()
sns.kdeplot(residuals_best_tree, shade=True)
plt.title("Residuals Kernel Density Plot for the Best Decision Tree")
plt.xlabel("Residuals")
plt.ylabel("Density")
plt.show()

# response...

#9

# response...

#Q3

#1
import pandas as pd

df2 = pd.read_csv('contraception_hw.csv')

method_counts = df2['method'].value_counts()
print("Method counts:")
print(method_counts)

cross_tabulation = pd.crosstab(df2['method'], df2['numberChildren'])
print("Cross-tabulation of method and numberChildren:")
print(cross_tabulation)

total_children_no_contraception = cross_tabulation.loc[1].sum()
total_children_contraception = cross_tabulation.loc[2:3].sum().sum()

print("\nTotal number of children:")
print(f"No Contraception: {total_children_no_contraception}")
print(f"With Contraception: {total_children_contraception}")

average_children_no_contraception = (cross_tabulation.loc[1] * cross_tabulation.columns).sum() / cross_tabulation.loc[1].sum()
average_children_contraception = (cross_tabulation.loc[2:3] * cross_tabulation.columns).sum().sum() / cross_tabulation.loc[2:3].sum().sum()

print("\nAverage number of children:")
print(f"No Contraception: {average_children_no_contraception:.2f}")
print(f"With Contraception: {average_children_contraception:.2f}")

# response...

#2
from sklearn.model_selection import train_test_split

X = df2.drop('method', axis=1)
y = df2['method']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#3
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

reg_tree = DecisionTreeRegressor(random_state=42)
reg_tree.fit(X, y)

plt.figure()
plot_tree(reg_tree)
plt.show()

#4
from sklearn.tree import DecisionTreeClassifier

class_tree = DecisionTreeClassifier(random_state=42)
class_tree.fit(X_train, y_train)

plt.figure()
plot_tree(class_tree)
plt.show()
