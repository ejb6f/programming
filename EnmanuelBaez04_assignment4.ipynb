#Q1.


#1
import pandas as pd
import numpy as np
df1 = pd.read_csv('cars_hw.csv')

no_owner_mapping = {'1st': 1, '2nd': 2, '3rd': 3}
df1['No_of_Owners'] = df1['No_of_Owners'].replace(no_owner_mapping)
df1 = df1.iloc[:, 1:]

#2
import seaborn as sns
import matplotlib.pyplot as plt

#price_summary = df1['Price'].describe()
#plt.figure(figsize = (8, 5))
#sns.kdeplot(df1['Price'], shade = True)
#plt.title('Kernel Density Plot for Price')
#plt.xlabel('Price')
#plt.ylabel('Density')
#plt.show()

#price_by_make = df1.groupby('Make')['Price'].describe()
#plt.figure(figsize = (12, 8))
#sns.kdeplot(data = df1, x = 'Price', hue = 'Make', common_norm = False, multiple = 'stack', fill = True)
#plt.title('Grouped Kernel Density Plot by Make')
#plt.xlabel('Price')
#plt.ylabel('Density')
#plt.legend(title = 'Make', loc = 'upper right')
#plt.show()

# response...

print(df1.dtypes)

#3
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

train_set, test_set = train_test_split(df1, test_size = 0.2, random_state = 42)

#4
numeric_variables = ['Make_Year', 'Mileage_Run', 'No_of_Owners', 'Seating_Capacity']
X_numeric = df1[numeric_variables]
y = df1['Price']
X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)
model_numeric = LinearRegression()
model_numeric.fit(X_train_numeric, y_train)
y_pred_numeric = model_numeric.predict(X_test_numeric)
r2_numeric = r2_score(y_test, y_pred_numeric)
rmse_numeric = np.sqrt(mean_squared_error(y_test, y_pred_numeric))
print("Numeric R-squared:" + str(r2_numeric))
print("Numeric Root mean squared error:" + str(rmse_numeric))

categorical_columns = ['Make', 'Color', 'Body_Type', 'Fuel_Type', 'Transmission', 'Transmission_Type']
df1_encoded = pd.get_dummies(df1, columns=categorical_columns, drop_first=True)
X_categorical = df1_encoded.drop(columns=['Price'])
X_train_categorical, X_test_categorical, y_train, y_test = train_test_split(X_categorical, y, test_size=0.2, random_state=42)
model_categorical = LinearRegression()
model_categorical.fit(X_train_categorical, y_train)
y_pred_categorical = model_categorical.predict(X_test_categorical)
r2_categorical = r2_score(y_test, y_pred_categorical)
rmse_categorical = np.sqrt(mean_squared_error(y_test, y_pred_categorical))
print("Categorical R-squared:" + str(r2_categorical))
print("Categorical Root mean squared error:" + str(rmse_categorical))

X_combined = pd.concat([X_numeric, X_categorical], axis=1)
X_train_combined, X_test_combined, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)
model_combined = LinearRegression()
model_combined.fit(X_train_combined, y_train)
y_pred_combined = model_combined.predict(X_test_combined)
r2_combined = r2_score(y_test, y_pred_combined)
rmse_combined = np.sqrt(mean_squared_error(y_test, y_pred_combined))
print("Combined R-squared:" + str(r2_combined))
print("Combined Root mean squared error:" + str(rmse_combined))

# response...

#5

from sklearn.preprocessing import PolynomialFeatures

degrees = range(1, 4)
r2_scores = []
rmse_values = []

for degree in degrees:
    poly = PolynomialFeatures(degree=degree)
    X_train_poly = poly.fit_transform(X_train_categorical)
    X_test_poly = poly.transform(X_test_categorical)
    model_poly = LinearRegression()
    model_poly.fit(X_train_poly, y_train)
    y_pred_poly = model_poly.predict(X_test_poly)
    r2_poly = r2_score(y_test, y_pred_poly)
    rmse_poly = np.sqrt(mean_squared_error(y_test, y_pred_poly))
    r2_scores.append(r2_poly)
    rmse_values.append(rmse_poly)
    print(f"Degree {degree}: R-squared (R2): {r2_poly:.4f}, RMSE: {rmse_poly:.2f}")

negative_r2_degree = None
for i, r2_score in enumerate(r2_scores):
    if r2_score < 0:
        negative_r2_degree = degrees[i]
        break
print(f"R-squared goes negative at degree {negative_r2_degree}")

# response...

#6

poly_reg = make_pipeline(PolynomialFeatures(1), LinearRegression())
poly_reg.fit(X_train_combined, y_train)
y_pred_poly = poly_reg.predict(X_test_combined)
plt.figure()
plt.scatter(y_test, y_pred_poly, alpha=0.5)
plt.title(f"Predicted vs. True Values for Polynomial Degree {1}")
plt.xlabel("True Values")
plt.ylabel("Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.show()

residuals_poly = y_test - y_pred_poly

plt.figure()
sns.kdeplot(residuals_poly, shade=True)
plt.title(f"Residuals Kernel Density Plot for Polynomial Degree {1}")
plt.xlabel("Residuals")
plt.ylabel("Density")
plt.show()

# response...

#7

from sklearn.tree import DecisionTreeRegressor

# Define a range of max_depth values to experiment with
depth_range = range(1, 11)  # You can adjust the range as needed

# Store results for analysis
r2_scores = []
rmse_scores = []

for depth in depth_range:
    # Create and fit the decision tree model
    tree_reg = DecisionTreeRegressor(max_depth=depth, random_state=42)
    tree_reg.fit(X_train_combined, y_train)

    # Predict on the test set
    y_pred_tree = tree_reg.predict(X_test_combined)

    # Evaluate the model
    r2_tree_result = r2_score(y_test, y_pred_tree)
    rmse_tree_result = np.sqrt(mean_squared_error(y_test, y_pred_tree))

    # Store results
    r2_scores.append(r2_tree_result)
    rmse_scores.append(rmse_tree_result)

    # Print results
    print(f"Max Depth {depth}: R-squared: {r2_tree_result}, RMSE: {rmse_tree_result}")

# Plot the results
plt.figure(figsize=(10, 6))

# Plot R-squared
plt.subplot(2, 1, 1)
plt.plot(depth_range, r2_scores, marker='o')
plt.title("R-squared vs. Max Depth")
plt.xlabel("Max Depth")
plt.ylabel("R-squared")

# Plot RMSE
plt.subplot(2, 1, 2)
plt.plot(depth_range, rmse_scores, marker='o', color='orange')
plt.title("RMSE vs. Max Depth")
plt.xlabel("Max Depth")
plt.ylabel("RMSE")

plt.tight_layout()
plt.show()
