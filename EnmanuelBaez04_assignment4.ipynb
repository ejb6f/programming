print("Question 1:")

print("1.1:")
import pandas as pd
import numpy as np
df1 = pd.read_csv('cars_hw.csv')

no_owner_mapping = {'1st': 1, '2nd': 2, '3rd': 3}
df1['No_of_Owners'] = df1['No_of_Owners'].replace(no_owner_mapping)
df1 = df1.iloc[:, 1:]

print("\n1.2:")
import seaborn as sns
import matplotlib.pyplot as plt

price_summary = df1['Price'].describe()
plt.figure()
sns.kdeplot(df1['Price'], shade = True)
plt.title('Kernel Density Plot for Price')
plt.xlabel('Price')
plt.ylabel('Density')
plt.show()

price_by_make = df1.groupby('Make')['Price'].describe()
plt.figure()
sns.kdeplot(data = df1, x = 'Price', hue = 'Make', common_norm = False, multiple = 'stack', fill = True)
plt.title('Grouped Kernel Density Plot by Make')
plt.xlabel('Price')
plt.ylabel('Density')
plt.legend(title = 'Make', loc = 'upper right')
plt.show()

average_price_by_make = df1.groupby('Make')['Price'].mean()

most_expensive_make = average_price_by_make.idxmax()
highest_average_price = average_price_by_make.max()
rounded_average_price = round(highest_average_price, 2)
mean_price_all_cars = df1['Price'].mean()
rounded_mean_price = round(mean_price_all_cars, 2)

print(f"The most expensive make is '{most_expensive_make}' with an average price of ${rounded_average_price}.")
print(f"The mean price of a cars is ${rounded_mean_price}, making it the general price point.")

print("\n1.3:")
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

train_set, test_set = train_test_split(df1, test_size = 0.2, random_state = 42)

print("\n1.4:")
numeric_variables = ['Make_Year', 'Mileage_Run', 'No_of_Owners', 'Seating_Capacity']
X_numeric = df1[numeric_variables]
y = df1['Price']
X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)
model_numeric = LinearRegression()
model_numeric.fit(X_train_numeric, y_train)
y_pred_numeric = model_numeric.predict(X_test_numeric)
r2_numeric = r2_score(y_test, y_pred_numeric)
rmse_numeric = np.sqrt(mean_squared_error(y_test, y_pred_numeric))
print("Numeric R-squared:" + str(r2_numeric))
print("Numeric Root mean squared error:" + str(rmse_numeric))

categorical_columns = ['Make', 'Color', 'Body_Type', 'Fuel_Type', 'Transmission', 'Transmission_Type']
df1_encoded = pd.get_dummies(df1, columns=categorical_columns, drop_first=True)
X_categorical = df1_encoded.drop(columns=['Price'])
X_train_categorical, X_test_categorical, y_train, y_test = train_test_split(X_categorical, y, test_size=0.2, random_state=42)
model_categorical = LinearRegression()
model_categorical.fit(X_train_categorical, y_train)
y_pred_categorical = model_categorical.predict(X_test_categorical)
r2_categorical = r2_score(y_test, y_pred_categorical)
rmse_categorical = np.sqrt(mean_squared_error(y_test, y_pred_categorical))
print("\nCategorical R-squared:" + str(r2_categorical))
print("Categorical Root mean squared error:" + str(rmse_categorical))

X_combined = pd.concat([X_numeric, X_categorical], axis=1)
X_train_combined, X_test_combined, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)
model_combined = LinearRegression()
model_combined.fit(X_train_combined, y_train)
y_pred_combined = model_combined.predict(X_test_combined)
r2_combined = r2_score(y_test, y_pred_combined)
rmse_combined = np.sqrt(mean_squared_error(y_test, y_pred_combined))
print("\nCombined R-squared:" + str(r2_combined))
print("Combined Root mean squared error:" + str(rmse_combined))

print("\nThe model that performs the best between numeric and categorical one is the categorical one, as it has a higher R-squared value.")
print("In contrast, the joint model performs practically the same as the categorical model, with a nearly identical R-squared value.")

print("\n1.5:")

from sklearn.preprocessing import PolynomialFeatures

degrees = range(1, 3)
r2_scores = []
rmse_values = []

for degree in degrees:
    poly = PolynomialFeatures(degree=degree)
    X_train_poly = poly.fit_transform(X_train_categorical)
    X_test_poly = poly.transform(X_test_categorical)
    model_poly = LinearRegression()
    model_poly.fit(X_train_poly, y_train)
    y_pred_poly = model_poly.predict(X_test_poly)
    r2_poly = r2_score(y_test, y_pred_poly)
    rmse_poly = np.sqrt(mean_squared_error(y_test, y_pred_poly))
    r2_scores.append(r2_poly)
    rmse_values.append(rmse_poly)
    print(f"Degree {degree}: R-squared (R2): {r2_poly:.4f}, RMSE: {rmse_poly:.2f}")

print("\nAs the degree of expansion increase, R-squared decreases.")

negative_r2_degree = None
for i, r2_score in enumerate(r2_scores):
    if r2_score < 0:
        negative_r2_degree = degrees[i]
        break
print(f"R-squared goes negative at degree {negative_r2_degree}.")
print("For the best model with expanded features, R-squared is 0.8197 and RMSE is 143815.30. These values are nearly the same as the best model from the previous question.")

print("\n1.6:")
from sklearn.pipeline import make_pipeline

poly_reg = make_pipeline(PolynomialFeatures(1), LinearRegression())
poly_reg.fit(X_train_combined, y_train)
y_pred_poly = poly_reg.predict(X_test_combined)
plt.figure()
plt.scatter(y_test, y_pred_poly, alpha=0.5)
plt.title(f"Predicted vs. True Values for Polynomial Degree {1}")
plt.xlabel("True Values")
plt.ylabel("Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.show()

residuals_poly = y_test - y_pred_poly

plt.figure()
sns.kdeplot(residuals_poly, shade=True)
plt.title(f"Residuals Kernel Density Plot for Polynomial Degree {1}")
plt.xlabel("Residuals")
plt.ylabel("Density")
plt.show()

print("The predicted values and the true values do roughly line up along the diagonal.")
print("The residuals do look roughly bell-shaped around zero.")

print("\nThe strengths of the model are that it that it provides comprehensive and expansive analysis of the data, conduting multiple analytical processes of both numeric and categorical variables.")
print("The weakness of the model is its complexity, as it presents information in a not-so-digestable fashion, making it harder for viewers to interpret.")

print("\n1.7:")

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error

depth_range = range(1, 16) 
r2_scores = []
rmse_scores = []

for depth in depth_range:
    tree_reg = DecisionTreeRegressor(max_depth=depth, random_state=42)
    tree_reg.fit(X_train_combined, y_train)
    y_pred_tree = tree_reg.predict(X_test_combined)
    r2_tree_result = r2_score(y_test, y_pred_tree)
    rmse_tree_result = np.sqrt(mean_squared_error(y_test, y_pred_tree))
    r2_scores.append(r2_tree_result)
    rmse_scores.append(rmse_tree_result)

    print(f"Max Depth {depth}: R-squared: {r2_tree_result}, RMSE: {rmse_tree_result}")

best_depth_index = np.argmax(r2_scores)
best_depth = depth_range[best_depth_index]

print(f"\nThe best results are obtained with a max depth of {best_depth}, with an R-squared of {r2_scores[best_depth_index]} and a RMSE of {rmse_scores[best_depth_index]}.")

print("\n1.8:")

best_tree = DecisionTreeRegressor(max_depth=9, random_state=42)
best_tree.fit(X_train_combined, y_train)
y_pred_best_tree = best_tree.predict(X_test_combined)

plt.figure()
plt.scatter(y_test, y_pred_best_tree, alpha=0.5)
plt.title("Predicted vs. True Values for the Best Decision Tree")
plt.xlabel("True Values")
plt.ylabel("Predicted Values")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.show()

residuals_best_tree = y_test - y_pred_best_tree

plt.figure()
sns.kdeplot(residuals_best_tree, shade=True)
plt.title("Residuals Kernel Density Plot for the Best Decision Tree")
plt.xlabel("Residuals")
plt.ylabel("Density")
plt.show()

print("The predicted values and the true values do roughly line up along the diagonal.")
print("The residuals do look roughly bell-shaped around zero.")

print("\n1.9:")

print("Which model --- linear model or classification and regression tree --- has better performance on the test set")
print("The model has a better perfomance on the test set")
